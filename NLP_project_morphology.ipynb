{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модуль сегментации на предложения, токенизации, лемматизации и морфологического анализа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Использование библиотек проекта *Natasha*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обработки нашего корпуса был использован набор Python-библиотек для обработки текстов на естественном русском языке **проекта Natasha**. Мы специально используем не базовую компактную библиотеку *natasha*, a отдельные профессиональные библиотеки проекта, качество и производительность которых выше, да и вообще это было вполне удобно. Итак, вот они: \n",
    "  * **Slovnet** — компактные модели для обработки естественного русского языка; нами использовался морфологический теггер.\n",
    "  * **Razdel** — инструмент для сегментации текста на предложения и токены.\n",
    "  * **Navec** — набор компактных предобученных эмбеддингов для русского языка.\n",
    "  * **Natasha** — из неё мы взяли лемматизатор, так как он, к нашему удивлению, не встроен в Slovnet.\n",
    "К тому же, *Slovnet* и *Navec* были обучены на новостях, а мы работаем с публицистикой, пусть и не совсем новостной."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### База данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предложения (в формате *id_sent, id_text, sent*) и токены (в формате *id_token, id_sent, id_text, token, lemma, pos, morph*)  записываются в таблицы *df_sents* и *df_tokens* базы данных *project_nlp.db* соответственно. Важно сказать, что кода для создания таблиц нет, так как они создавались с помощью приложения *DB Browser for SQLite*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Для тестирования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дорогие преподаватели, если вы хотите протестировать следующие ячейки кода, советую вам закомментировать и раскомментировать строки, у которых указан соответствующий комментарий. В базе данных находится более 600 000 тысяч словоупотреблений, она относительно долго собиралась."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Морфологический анализ и лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция *tokens(chunk, id_sents, id_text)* производит морфологический анализ токена с помощью *Slovnet*, основанный на контексте, находит лемму с помощью *morph_vocab.lemmatize()* из *Natasha* и делает запись в базу данных в формате описанном выше. На вход получает список *chunk*, представляющий собой список предложений, представленных в виде списков токенов (извините за тавтологию), а также id текста и предложения.\n",
    "Теги частей речи и морфологические теги являются [Universal POS tags](https://universaldependencies.org/u/pos/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens(chunk, id_sents, id_text):\n",
    "    x = morph.map(chunk)\n",
    "    tokens = []\n",
    "    for id_s in id_sents:\n",
    "        markup = next(x)\n",
    "        count_tok = 0\n",
    "        for token in markup.tokens:\n",
    "            t = list(token)\n",
    "            if t[1] != 'PUNCT':\n",
    "                lemma = morph_vocab.lemmatize(t[0], t[1], t[2])\n",
    "                t.append(lemma)\n",
    "                grammemes = t.pop(2)\n",
    "                t.insert(2, ','.join([':'.join([x, y]) for x, y in list(grammemes.items())]))\n",
    "                t.append(id_text)\n",
    "                t.append(id_s)\n",
    "                t.append(counter_token)\n",
    "                token_to_db = tuple([t[6], t[5], t[4], t[0], t[3], t[1], t[2]])\n",
    "                # print(token_to_db)\n",
    "                c.execute('INSERT INTO df_tokens VALUES (?, ?, ?, ?, ?, ?, ?)', token_to_db) # это можно закомментировать для тестирования\n",
    "                counter_token += 1\n",
    "                con.commit() # это можно закомментировать для тестирования\n",
    "                count_tok += 1\n",
    "            markup = next(morph.map(chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сегментация на предложения и токенизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция *sents_with_tokens(row)* производит сегментацию текста на предложения и токенизацию с помощью инструментов *Razdel*, а затем вписывает информацию о предложении в базу данных в форфмате, описанном выше. После этого вызывает функцию *tokens(chunk, id_sents, id_text)*. На вход получает кортеж из id текста и самого текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sents_with_tokens(row):\n",
    "    counter = 0\n",
    "    sents = list(sentenize(row[1]))\n",
    "    chunk = []\n",
    "    sents_final = []\n",
    "    id_sents = []\n",
    "    for s in sents:\n",
    "        sent_to_db = []\n",
    "        id_sent = row[0] + '_' + str(counter)\n",
    "        id_sents.append(id_sent)\n",
    "        sent_to_db.append(id_sent)\n",
    "        sent_to_db.append(row[0])\n",
    "        sent_to_db.append(list(s)[2])\n",
    "        sent_to_db = tuple(sent_to_db)\n",
    "        # print(sent_to_db)\n",
    "        c.execute('INSERT INTO df_sents VALUES (?, ?, ?)', sent_to_db) # это можно закомментировать для тестирования\n",
    "        con.commit() # это можно закомментировать для тестирования\n",
    "        tokens_ch = [_.text for _ in tokenize(s.text)]\n",
    "        chunk.append(tokens_ch)\n",
    "        counter += 1\n",
    "    tokens(chunk, id_sents, row[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Активация функций и заполнение базы данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подключаемся к базе данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect('project_nlp.db')\n",
    "c = con.cursor()\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь вызывается объект *MorphVocab()* для лемматизации и загружаются модели ([эмбеддинги](https://github.com/natasha/navec) и [морфоанализатор](https://github.com/natasha/slovnet); второй будет приложен на GitHub, первый не получается приложить, так как он весит 25 Мб, но его можно скачать по ссылке) для работы морфолгоческого анализатора. Также выкачивается список пар (*id_text, text*) для последующей обработки. Делать это без такого списка у нас не получилось. Видимо, из-за того, что с такими объёмами информации *sqlite3* работает нестабильно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from razdel import sentenize\n",
    "from razdel import tokenize\n",
    "from navec import Navec\n",
    "from slovnet import Morph\n",
    "from natasha import MorphVocab\n",
    "\n",
    "navec = Navec.load('navec_news_v1_1B_250K_300d_100q.tar')\n",
    "morph = Morph.load('slovnet_morph_news_v1.tar', batch_size=4)\n",
    "morph.navec(navec)\n",
    "\n",
    "morph_vocab = MorphVocab()\n",
    "\n",
    "rows = []\n",
    "counter_token = 0\n",
    "for row in c.execute('SELECT id_text, text FROM df_texts'):\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь для каждого текста из базы данных активируется функция *sents_with_tokens(row)* (и, соответственно, *tokens(chunk, id_sents, id_text)*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 259/259 [1:02:27<00:00, 14.47s/it]\n"
     ]
    }
   ],
   "source": [
    "for r in tqdm(rows):\n",
    "    sents_with_tokens(r)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Закрываем базу данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
